"""
GPU Utilization Test for AlphaTrade System
==========================================

Identifies bottlenecks and tests maximum GPU throughput.
"""

import torch
import numpy as np
import time
from alphago_trading_system import (
    Config, build_network, AlphaTradeNet,
    DEVICE, HAS_CUDA, NUM_FEATURES
)


def test_network_throughput():
    """Test maximum forward pass throughput on GPU."""
    print("\n" + "="*70)
    print("TEST 1: Network Forward Pass Throughput")
    print("="*70)

    cfg = Config()
    net = build_network(cfg).eval()

    # Test different batch sizes
    batch_sizes = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096]

    print(f"Device: {DEVICE}")
    print(f"Input dim: {cfg.obs_dim}")
    print(f"Network params: {sum(p.numel() for p in net.parameters()):,}")

    results = []
    for bs in batch_sizes:
        # Create dummy batch
        dummy_input = torch.randn(bs, cfg.obs_dim, device=DEVICE)

        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = net(dummy_input)

        if HAS_CUDA:
            torch.cuda.synchronize()

        # Benchmark
        n_iters = 100
        t0 = time.time()
        with torch.no_grad():
            for _ in range(n_iters):
                _ = net(dummy_input)

        if HAS_CUDA:
            torch.cuda.synchronize()

        elapsed = time.time() - t0
        throughput = (bs * n_iters) / elapsed

        # Check memory
        if HAS_CUDA:
            mem_mb = torch.cuda.memory_allocated() / 1024**2
            print(f"Batch {bs:>4}: {throughput:>10,.0f} inferences/sec | GPU: {mem_mb:>6.0f} MB")
        else:
            print(f"Batch {bs:>4}: {throughput:>10,.0f} inferences/sec")

        results.append({
            'batch_size': bs,
            'throughput': throughput,
        })

        # Stop if we hit OOM
        if HAS_CUDA and torch.cuda.memory_allocated() / 1024**3 > 20:
            print(f"\nStopping: Approaching VRAM limit")
            break

    # Find optimal batch size
    best = max(results, key=lambda x: x['throughput'])
    print(f"\n[RESULT] Optimal batch size: {best['batch_size']} ({best['throughput']:,.0f} inf/sec)")

    return best['batch_size']


def test_parallel_envs():
    """Test maximum number of parallel environments."""
    print("\n" + "="*70)
    print("TEST 2: Parallel Environment Capacity")
    print("="*70)

    cfg = Config()

    # Simulate environment observations
    n_envs_options = [16, 32, 64, 128, 256]

    for n_envs in n_envs_options:
        total_obs = n_envs * cfg.obs_dim

        # Simulate environment batch
        obs_batch = torch.randn(n_envs, cfg.obs_dim, device=DEVICE)

        if HAS_CUDA:
            mem_gb = torch.cuda.memory_allocated() / 1024**3
            max_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            utilization = (mem_gb / max_mem_gb) * 100

            print(f"n_envs={n_envs:>3}: {mem_gb:>5.2f} GB / {max_mem_gb:>5.2f} GB ({utilization:>5.1f}%)")

            if mem_gb > 20:  # Leave 4GB headroom
                print(f"  [LIMIT] Max recommended: {n_envs}")
                break
        else:
            print(f"n_envs={n_envs:>3}: CPU mode")

        del obs_batch
        if HAS_CUDA:
            torch.cuda.empty_cache()


def test_current_config():
    """Report current configuration and utilization."""
    print("\n" + "="*70)
    print("CURRENT CONFIGURATION")
    print("="*70)

    cfg = Config()

    print(f"n_envs:              {cfg.n_envs}")
    print(f"batch_size:          {cfg.batch_size}")
    print(f"mcts_rollouts:       {cfg.mcts_rollouts}")
    print(f"mcts_batch_size:     {cfg.mcts_batch_size}")
    print(f"use_compile:         {cfg.use_compile}")
    print(f"use_amp:             {cfg.use_amp}")
    print(f"total_steps/iter:    {cfg.total_timesteps_per_iter:,}")
    print(f"iterations:          {cfg.n_iterations}")

    if HAS_CUDA:
        props = torch.cuda.get_device_properties(0)
        print(f"\nGPU:                 {props.name}")
        print(f"VRAM:                {props.total_memory / 1024**3:.1f} GB")
        print(f"CUDA Cores:          {props.multi_processor_count * 128:,}")
        print(f"Compute Capability:  {props.major}.{props.minor}")

    # Estimate utilization
    obs_memory = cfg.n_envs * cfg.obs_dim * 4 / 1024**2  # 4 bytes per float32
    print(f"\nEstimated obs memory: {obs_memory:.1f} MB")
    print(f"Utilization:          LOW (estimate <10%)")


def main():
    print("\n" + "="*70)
    print("GPU UTILIZATION ANALYSIS - AlphaTrade System")
    print("="*70)

    if not HAS_CUDA:
        print("\n[ERROR] No CUDA GPU detected!")
        print("This system requires GPU for optimal performance.")
        return

    # Test current config
    test_current_config()

    # Test network throughput
    optimal_batch = test_network_throughput()

    # Test parallel envs
    test_parallel_envs()

    # Recommendations
    print("\n" + "="*70)
    print("RECOMMENDATIONS")
    print("="*70)

    cfg = Config()

    print(f"\n[1] Increase n_envs: {cfg.n_envs} -> 64-128")
    print(f"    Impact: 4-8x more parallel environments")
    print(f"    VRAM: ~2-4 GB (plenty of headroom)")

    print(f"\n[2] Increase batch_size: {cfg.batch_size} -> {optimal_batch}")
    print(f"    Impact: {optimal_batch/cfg.batch_size:.1f}x better GPU utilization")

    print(f"\n[3] Parallelize MCTS rollouts (CODE CHANGE)")
    print(f"    Current: 32 rollouts Ã— sequential")
    print(f"    Proposed: Batch all rollouts in parallel")
    print(f"    Impact: ~10-20x speedup on MCTS phase")

    print(f"\n[4] Increase mcts_batch_size: {cfg.mcts_batch_size} -> 128")
    print(f"    Impact: 4x more MCTS roots processed per iteration")

    print(f"\nESTIMATED TOTAL SPEEDUP: 5-10x on RL training")
    print(f"Combined with CV parallelization (15-20x): 75-200x total")
    print(f"New backtest time: 1-3 MINUTES (from 2-3 hours)")


if __name__ == "__main__":
    main()
